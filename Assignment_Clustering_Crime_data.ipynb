{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfab9a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from sklearn.metrics import silhouette_score as sil, calinski_harabasz_score as chs, silhouette_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4a971e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supressing Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94a0c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data display customization\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974ec303",
   "metadata": {},
   "source": [
    "##  Content\n",
    "This data set contains statistics, in arrests per 100,000 residents for assault, murder, and rape in each of the 50 US states in 1973. Also given is the percent of the population living in urban areas.This is a systematic approach for identifying and analyzing patterns and trends in crime using USArrest dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847d0feb",
   "metadata": {},
   "source": [
    "A data frame with 50 observations on 4 variables.\n",
    "\n",
    "+ Murder is numeric and Murder arrests (per 100,000)\n",
    "+ Assault is numeric and Assault arrests (per 100,000)\n",
    "+ UrbanPop is numeric and UrbanPop arrests (per 100,000)\n",
    "+ Rape is numeric and Rape arrests (per 100,000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a86035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dataset\n",
    "data = pd.read_csv('crime_data.csv')\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85279682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming the Unnamed: 0 Column into States\n",
    "data.rename({'Unnamed: 0':'States'}, axis=1, inplace=True)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66257460",
   "metadata": {},
   "source": [
    "##  Data Exploration <a class=\"anchor\" id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7a4739",
   "metadata": {},
   "source": [
    "#### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ce198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecc3817",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e33aa77",
   "metadata": {},
   "source": [
    "###  Missing Values <a class=\"anchor\" id=\"2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929a359f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# As a part of the Data cleansing we check the data for any missing/ na values\n",
    "# null count for columns\n",
    "\n",
    "null_count_col = data.isnull().sum().value_counts(ascending=False)\n",
    "\n",
    "# null percentage for columns\n",
    "\n",
    "null_percent_col = (data.isnull().sum() * 100 / len(data)).value_counts(ascending=False)\n",
    "\n",
    "print(\"Null Count for Columns:\\n\\n\", null_count_col, \"\\n\")\n",
    "print(\"Null Percentage for Columns:\\n\\n\", null_percent_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e46279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# null count for rows\n",
    "\n",
    "null_count_row = data.isnull().sum(axis=1).value_counts(ascending=False)\n",
    "\n",
    "# null percentage for rows\n",
    "\n",
    "null_percent_row = (data.isnull().sum(axis=1) * 100 / len(data)).value_counts(ascending=False)\n",
    "\n",
    "print(\"Null Count for Rows:\\n\\n\", null_count_row, \"\\n\")\n",
    "print(\"Null Percentage for Rows:\\n\\n\", null_percent_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a01f3a5",
   "metadata": {},
   "source": [
    "### Duplicated Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e14fef1",
   "metadata": {},
   "source": [
    "#### print the duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a294b892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additionally we check the data for any duplicate values, now this can be an optional check depending on the data being used\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be5c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dd6a11",
   "metadata": {},
   "source": [
    "#### There are no missing / Null and Duplicated  values  either in columns or rows, so we can move on to the next step, which is Exploratory Data Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f870ae30",
   "metadata": {},
   "source": [
    "##  Exploratory Data Analysis<a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af39509e",
   "metadata": {},
   "source": [
    "### Lets analyze the features by creating histograms to understand the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf853100",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in data.columns:\n",
    "    data=data.copy()\n",
    "    data[feature].hist(bins=10)\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(feature)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777747ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b2ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Kernel Density for every feature, singled out\n",
    "\n",
    "#for n in data.columns:\n",
    "#    print(n)\n",
    "#    sns.kdeplot(data[n])\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d1b0aa",
   "metadata": {},
   "source": [
    "### 3.1 Outliers Detection<a class=\"anchor\" id=\"3.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b2655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a234b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ot=data.copy() \n",
    "fig, axes=plt.subplots(4,1,figsize=(12,8),sharex=False,sharey=False)\n",
    "sns.boxplot(x='Murder',data=ot,palette='crest',ax=axes[0])\n",
    "sns.boxplot(x='Assault',data=ot,palette='crest',ax=axes[1])\n",
    "sns.boxplot(x='UrbanPop',data=ot,palette='crest',ax=axes[2])\n",
    "sns.boxplot(x='Rape',data=ot,palette='crest',ax=axes[3])\n",
    "plt.tight_layout(pad=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c835ca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot for every feature in the same graph\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.boxplot(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390fc891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use sqrt() to see more clearly despite the outliers\n",
    "\n",
    "#plt.figure(figsize=(12,8))\n",
    "#sns.boxplot(data=np.sqrt(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7da1b43",
   "metadata": {},
   "source": [
    "## 4. Data Visualization<a class=\"anchor\" id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60827fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation heatmap\n",
    "\n",
    "f,ax = plt.subplots(figsize=(18,12))\n",
    "sns.heatmap(data.corr(), annot=True, linewidths =.5, fmt ='.1f',ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d74f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a277b638",
   "metadata": {},
   "source": [
    "Unique points in this correlation matrix:\n",
    "\n",
    "+ Assault is positively correlated with Murder\n",
    "+ Assualt is positively correlated with Rape\n",
    "+ Rape is positively correlated with Murder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecc5bbb",
   "metadata": {},
   "source": [
    "### 4.1 Murder Rate<a class=\"anchor\" id=\"4.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be09436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "# make barplot and sort bars\n",
    "sns.barplot(x='States',\n",
    "            y=\"Murder\", \n",
    "            data=data, \n",
    "            order=data.sort_values('Murder').States)\n",
    "# set labels\n",
    "plt.xlabel(\"States\", size=15)\n",
    "plt.ylabel(\"Murder Rate\", size=15)\n",
    "plt.yticks(rotation = 0, fontsize = 14)\n",
    "plt.xticks(rotation = 90, fontsize = 16)\n",
    "plt.title(\"Murder Rate in US State wise\", size=18)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "# make barplot and sort bars\n",
    "sns.barplot(x='States',\n",
    "            y=\"UrbanPop\", \n",
    "            data=data, \n",
    "            order=data.sort_values('UrbanPop').States)\n",
    "# set labels\n",
    "plt.xlabel(\"States\", size=15)\n",
    "plt.ylabel(\"Urban Population Rate\", size=15)\n",
    "plt.yticks(rotation = 0, fontsize = 14)\n",
    "plt.xticks(rotation = 90, fontsize = 16)\n",
    "plt.title(\"Urban Population Rate in US State wise\", size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da09c116",
   "metadata": {},
   "source": [
    "#### Observation: There is no direct relation between Murder Rate and Urban Population but you can see a less number of Murder rate can be seen North Dakota as it Urban population is less compare to Florida and has the opposite effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070f9e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Top 10 States with Highest Murder Rate')\n",
    "data.sort_values('Murder',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a90db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Top 10 States with Lowest Murder Rate')\n",
    "data.sort_values('Murder',ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc75b6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "murder = data.sort_values('Murder', ascending = False, ignore_index=True)\n",
    "murder.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e6e1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "ax = sns.barplot(x = murder.Murder[:10], y = murder.States[:10])\n",
    "plt.yticks(rotation = 0, fontsize = 14)\n",
    "plt.xticks(rotation = 45, fontsize = 12)\n",
    "plt.title('Top 10 Highest Murder Rate State Wise',  fontsize = 18, fontweight = 'bold')\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261a67bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "murder = data.sort_values('Murder', ascending = True,ignore_index=True)\n",
    "murder.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136a2b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "ax = sns.barplot(x = murder.Murder[:10], y = murder.States[:10])\n",
    "plt.yticks(rotation = 0, fontsize = 14)\n",
    "plt.xticks(rotation = 45, fontsize = 12)\n",
    "plt.title('Top 10 Lowest Murder Rate State Wise',  fontsize = 18, fontweight = 'bold')\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1523b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "plt.pie(murder.Murder[:10],\n",
    "       labels=murder.States[:10],\n",
    "       explode = [0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\n",
    "       autopct= '%.2f%%',\n",
    "       shadow= True,\n",
    "       startangle= 190,\n",
    "       textprops = {'size':'large',\n",
    "                   'fontweight':'bold',\n",
    "                    'rotation':'0',\n",
    "                   'color':'black'})\n",
    "#plt.legend(loc= 'best')\n",
    "plt.title(\"Top 10 Highest Murder Rate State Wise\", fontsize = 18, fontweight = 'bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc3dc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "plt.pie(murder.Murder[:10],\n",
    "       labels=murder.States[:10],\n",
    "       explode = [0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\n",
    "       autopct= '%.2f%%',\n",
    "       shadow= True,\n",
    "       startangle= 190,\n",
    "       textprops = {'size':'large',\n",
    "                   'fontweight':'bold',\n",
    "                    'rotation':'0',\n",
    "                   'color':'black'})\n",
    "#plt.legend(loc= 'best')\n",
    "plt.title(\"Top 10 Lowest Murder Rate State-Wise\", fontsize = 18, fontweight = 'bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1ccfb4",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "+ #### Highest Murder Rate : Georgia, Missisippi and Florida\n",
    "+ #### Lowest Murder Rate : North Dakota, New Hampshire, Idaho, Iowa, Maine, Vermont and Wisconsin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddceff82",
   "metadata": {},
   "source": [
    "### 4.2 Assault Rate<a class=\"anchor\" id=\"4.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6960482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "# make barplot and sort bars\n",
    "sns.barplot(x='States',\n",
    "            y=\"Assault\", \n",
    "            data=data, \n",
    "            order=data.sort_values('Assault').States)\n",
    "# set labels\n",
    "plt.xlabel(\"States\", size=15)\n",
    "plt.ylabel(\"Assault Rate\", size=15)\n",
    "plt.yticks(rotation = 0, fontsize = 14)\n",
    "plt.xticks(rotation = 90, fontsize = 16)\n",
    "plt.title(\"Assault Rate in US State wise\", size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcac4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Top 10 States with Highest Assault Rate')\n",
    "data.sort_values('Assault',ascending=False, ignore_index=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4758f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Top 10 States with Lowest Assault Rate')\n",
    "data.sort_values('Assault',ascending=True, ignore_index=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c489ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Assault = data.sort_values('Assault', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64431bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "ax = sns.barplot(x = Assault.Assault[:10], y = Assault.States[:10])\n",
    "plt.yticks(rotation = 0, fontsize = 14)\n",
    "plt.xticks(rotation = 45, fontsize = 12)\n",
    "plt.title('Top 10 Highest Assault Rate State Wise',  fontsize = 18, fontweight = 'bold')\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d620425",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "plt.pie(Assault.Assault[:10],\n",
    "       labels=Assault.States[:10],\n",
    "       explode = [0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\n",
    "       autopct= '%.2f%%',\n",
    "       shadow= True,\n",
    "       startangle= 190,\n",
    "       textprops = {'size':'large',\n",
    "                   'fontweight':'bold',\n",
    "                    'rotation':'0',\n",
    "                   'color':'black'})\n",
    "#plt.legend(loc= 'best')\n",
    "plt.title(\"Top 10 Highest Assault Rate States-wise\", fontsize = 18, fontweight = 'bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84418be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assault = data.sort_values('Assault', ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e81cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "ax = sns.barplot(x = assault.Assault[:10], y = assault.States[:10])\n",
    "plt.yticks(rotation = 0, fontsize = 14)\n",
    "plt.xticks(rotation = 45, fontsize = 12)\n",
    "plt.title('Top 10 Lowest Assault Rate State Wise',  fontsize = 18, fontweight = 'bold')\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a52db4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "plt.pie(assault.Assault[:10],\n",
    "       labels=assault.States[:10],\n",
    "       explode = [0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\n",
    "       autopct= '%.2f%%',\n",
    "       shadow= True,\n",
    "       startangle= 190,\n",
    "       textprops = {'size':'large',\n",
    "                   'fontweight':'bold',\n",
    "                    'rotation':'0',\n",
    "                   'color':'black'})\n",
    "#plt.legend(loc= 'best')\n",
    "plt.title(\"Top 10 Lowest Assault Rate States-wise\", fontsize = 18, fontweight = 'bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6806107c",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "+ #### Highest Assault Rate : North Carolina, Florida and Maryland\n",
    "+ #### Lowest Assault Rate : North Dakota, Hawaii and Vermont."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134396b6",
   "metadata": {},
   "source": [
    "### 4.3 Rape Rate<a class=\"anchor\" id=\"4.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f372936",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "# make barplot and sort bars\n",
    "sns.barplot(x='States',\n",
    "            y=\"Rape\", \n",
    "            data=data, \n",
    "            order=data.sort_values('Rape').States)\n",
    "# set labels\n",
    "plt.xlabel(\"States\", size=15)\n",
    "plt.ylabel(\"Rape Rate\", size=15)\n",
    "plt.yticks(rotation = 0, fontsize = 14)\n",
    "plt.xticks(rotation = 90, fontsize = 16)\n",
    "plt.title(\"Rape Rate in US State wise\", size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982caa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Top 10 States with Highest Rape Rate')\n",
    "data.sort_values('Rape',ascending=False, ignore_index=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a3651",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Top 10 States with Lowest Rape Rate')\n",
    "data.sort_values('Rape',ascending=True, ignore_index=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eae1d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rape = data.sort_values('Rape', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da05ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "ax = sns.barplot(x = Rape.Rape[:10], y = Rape.States[:10])\n",
    "plt.yticks(rotation = 0, fontsize = 14)\n",
    "plt.xticks(rotation = 45, fontsize = 12)\n",
    "plt.title('Top 10 Highest Rape Rate State Wise',  fontsize = 18, fontweight = 'bold')\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910dd385",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "plt.pie(Rape.Rape[:10],\n",
    "       labels=Rape.States[:10],\n",
    "       explode = [0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\n",
    "       autopct= '%.2f%%',\n",
    "       shadow= True,\n",
    "       startangle= 190,\n",
    "       textprops = {'size':'large',\n",
    "                   'fontweight':'bold',\n",
    "                    'rotation':'0',\n",
    "                   'color':'black'})\n",
    "#plt.legend(loc= 'best')\n",
    "plt.title(\"Top 10 Highest Rape Rate States-wise\", fontsize = 18, fontweight = 'bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59299058",
   "metadata": {},
   "outputs": [],
   "source": [
    "rape = data.sort_values('Rape', ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89b1cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "ax = sns.barplot(x = rape.Rape[:10], y = rape.States[:10])\n",
    "plt.yticks(rotation = 0, fontsize = 14)\n",
    "plt.xticks(rotation = 45, fontsize = 12)\n",
    "plt.title('Top 10 Lowest Rape Rate State Wise',  fontsize = 18, fontweight = 'bold')\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f456084",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "plt.pie(rape.Rape[:10],\n",
    "       labels=rape.States[:10],\n",
    "       explode = [0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\n",
    "       autopct= '%.2f%%',\n",
    "       shadow= True,\n",
    "       startangle= 190,\n",
    "       textprops = {'size':'large',\n",
    "                   'fontweight':'bold',\n",
    "                    'rotation':'0',\n",
    "                   'color':'black'})\n",
    "#plt.legend(loc= 'best')\n",
    "plt.title(\"Top 10 Lowest Rape Rate States-wise\", fontsize = 18, fontweight = 'bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d325a31",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "+ #### Highest Rape Rate : Nevada, Alaska and California\n",
    "+ #### Lowest Rape Rate : North Dakota, Maine and Rhode Island."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e86846",
   "metadata": {},
   "source": [
    "### 4.4 Urban Population Rate<a class=\"anchor\" id=\"4.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63088051",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97d94d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "# make barplot and sort bars\n",
    "sns.barplot(x='States',\n",
    "            y=\"UrbanPop\", \n",
    "            data=data, \n",
    "            order=data.sort_values('UrbanPop').States)\n",
    "# set labels\n",
    "plt.xlabel(\"States\", size=15)\n",
    "plt.ylabel(\"Urban Population Rate\", size=15)\n",
    "plt.yticks(rotation = 0, fontsize = 14)\n",
    "plt.xticks(rotation = 90, fontsize = 16)\n",
    "plt.title(\"Urban Population Rate in US State wise\", size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7888277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Top 10 States with Highest Urban Population Rate')\n",
    "data.sort_values('UrbanPop',ascending=False, ignore_index=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e804e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Top 10 States with Lowest Urban Population Rate')\n",
    "data.sort_values('UrbanPop',ascending=True, ignore_index=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6407ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "urbanpop = data.sort_values('UrbanPop', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5405b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "ax = sns.barplot(x = urbanpop.UrbanPop[:10], y = urbanpop.States[:10])\n",
    "plt.yticks(rotation = 0, fontsize = 14)\n",
    "plt.xticks(rotation = 45, fontsize = 12)\n",
    "plt.title('Top 10 Highest Urban Population Rate State Wise',  fontsize = 18, fontweight = 'bold')\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474b8380",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "plt.pie(urbanpop.UrbanPop[:10],\n",
    "       labels=urbanpop.States[:10],\n",
    "       explode = [0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\n",
    "       autopct= '%.2f%%',\n",
    "       shadow= True,\n",
    "       startangle= 190,\n",
    "       textprops = {'size':'large',\n",
    "                   'fontweight':'bold',\n",
    "                    'rotation':'0',\n",
    "                   'color':'black'})\n",
    "#plt.legend(loc= 'best')\n",
    "plt.title(\"Top 10 Highest Urban Population Rate States-wise\", fontsize = 18, fontweight = 'bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa3c2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "UrbanPop = data.sort_values('UrbanPop', ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c11e7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "ax = sns.barplot(x = UrbanPop.UrbanPop[:10], y = UrbanPop.States[:10])\n",
    "plt.yticks(rotation = 0, fontsize = 14)\n",
    "plt.xticks(rotation = 45, fontsize = 12)\n",
    "plt.title('Top 10 Lowest Urban Population Rate State Wise',  fontsize = 18, fontweight = 'bold')\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785aa1db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "plt.pie(UrbanPop.UrbanPop[:10],\n",
    "       labels=UrbanPop.States[:10],\n",
    "       explode = [0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\n",
    "       autopct= '%.2f%%',\n",
    "       shadow= True,\n",
    "       startangle= 190,\n",
    "       textprops = {'size':'large',\n",
    "                   'fontweight':'bold',\n",
    "                    'rotation':'0',\n",
    "                   'color':'black'})\n",
    "#plt.legend(loc= 'best')\n",
    "plt.title(\"Top 10 Lowest Urban Population Rate States-wise\", fontsize = 18, fontweight = 'bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eec12f",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "+ #### Highest Urban Population Rate : California, New Jersey and Rhode Island\n",
    "+ #### Lowest Urban Population Rate : Vermount, West Virginia and North Dakota."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6028aa",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing<a class=\"anchor\" id=\"5\"></a>\n",
    "\n",
    "### 5.1) Standardizing the Data<a class=\"anchor\" id=\"5.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d876b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.set_index('States')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90c62be",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = StandardScaler()\n",
    "std_df = standard_scaler.fit_transform(df)\n",
    "std_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f9522f",
   "metadata": {},
   "source": [
    "### 5.2) Normalizing the data<a class=\"anchor\" id=\"5.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa89200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Minmaxscaler for accuracy result comparison\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "minmax = MinMaxScaler()\n",
    "\n",
    "minmax_df = minmax.fit_transform(df)\n",
    "minmax_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da41fc7",
   "metadata": {},
   "source": [
    "## 6. KMeans Clustering<a class=\"anchor\" id=\"6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eb780b",
   "metadata": {},
   "source": [
    "### 6.1 Elbow Method for Determining Cluster Amount<a class=\"anchor\" id=\"6.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9302f334",
   "metadata": {},
   "source": [
    "### Standard Scaler Applied on Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0da4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_range = range(1,15)\n",
    "cluster_errors = []\n",
    "for num_clusters in cluster_range:\n",
    "    clusters = KMeans(num_clusters,n_init=10)\n",
    "    clusters.fit(std_df)\n",
    "    labels = clusters.labels_\n",
    "    centroids = clusters.cluster_centers_\n",
    "    cluster_errors.append(clusters.inertia_)\n",
    "clusters_df = pd.DataFrame({\"num_clusters\":cluster_range,\"cluster_errors\":cluster_errors})\n",
    "clusters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9b3bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# within-cluster sum-of-squares criterion \n",
    "# Use Elbow Graph to find optimum number of  clusters (K value) from K values range\n",
    "# The K-means algorithm aims to choose centroids that minimise the inertia, or within-cluster sum-of-squares criterion WCSS \n",
    "# random state can be anything from 0 to 42, but the same number to be used everytime,so that the results don't change. \n",
    "\n",
    "wcss=[]\n",
    "for i in range(1,9):\n",
    "    kmeans=KMeans(n_clusters=i,random_state=2)\n",
    "    kmeans.fit(std_df)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "# Plot K values range vs WCSS to get Elbow graph for choosing K (no. of clusters)\n",
    "plt.plot(range(1,9),wcss,color = 'black')\n",
    "plt.scatter(range(1,9),wcss,color='red')\n",
    "plt.title('Elbow Graph for Standard Scaler')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4a60ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "n_clusters = [2,3,4,5,6,7,8,9,10] # number of clusters\n",
    "clusters_inertia = [] # inertia of clusters\n",
    "s_scores = [] # silhouette scores\n",
    "\n",
    "for n in n_clusters:\n",
    "    KM_est = KMeans(n_clusters=n, init='k-means++').fit(std_df)\n",
    "    clusters_inertia.append(KM_est.inertia_)    # data for the elbow method\n",
    "    silhouette_avg = silhouette_score(std_df, KM_est.labels_)\n",
    "    s_scores.append(silhouette_avg) # data for the silhouette score method\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "ax = sns.lineplot(n_clusters, clusters_inertia, marker='o', ax=ax)\n",
    "ax.set_title(\"Elbow method\")\n",
    "ax.set_xlabel(\"number of clusters\")\n",
    "ax.set_ylabel(\"clusters inertia\")\n",
    "ax.axvline(4, ls=\"--\", c=\"red\")\n",
    "ax.axvline(5, ls=\"--\", c=\"red\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a7d2d0",
   "metadata": {},
   "source": [
    "##### There is no clear \"elbow\" visible. A choice of 4 or 5 clusters seems to be fair. Let's see the silhouette score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c4babe",
   "metadata": {},
   "source": [
    "### 6.2 Silhouette Score<a class=\"anchor\" id=\"6.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa7c27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. How many number of clusters? n_clusters?\n",
    "\n",
    "# Since true labels are not known..we will use Silhouette Coefficient (Clustering performance evaluation)\n",
    "# knee Elbow graph method\n",
    "\n",
    "\n",
    "# Instantiate a scikit-learn K-Means model. we will check for two diff hyperparameters value effect.\n",
    "model = KMeans(random_state=10, max_iter=500, init='k-means++')\n",
    "\n",
    "# Instantiate the KElbowVisualizer with the number of clusters and the metric\n",
    "visualizer = KElbowVisualizer(model, k=(2,20), metric='silhouette', timings=False)\n",
    "# Fit the data and visualize\n",
    "print('Elbow Plot for Standard Scaler data')\n",
    "visualizer.fit(std_df)    \n",
    "visualizer.poof()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4387a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "ax = sns.lineplot(n_clusters, s_scores, marker='o', ax=ax)\n",
    "ax.set_title(\"Silhouette score method\")\n",
    "ax.set_xlabel(\"number of clusters\")\n",
    "ax.set_ylabel(\"Silhouette score\")\n",
    "ax.axvline(2, ls=\"--\", c=\"red\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0857cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the elbow method, the ideal number of clusters to use was 6.\n",
    "# We will also use the Silhouette score to determine an optimal number.\n",
    "\n",
    "clust_list = [2,3,4,5,6,7,8,9]\n",
    "\n",
    "#  Silhouette score for stadardScaler applied on data.\n",
    "\n",
    "for n_clusters in clust_list:\n",
    "    clusterer1 = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    cluster_labels1 = clusterer1.fit_predict(std_df)\n",
    "    sil_score1= sil(std_df, cluster_labels1)\n",
    "    print(\"For n_clusters =\", n_clusters,\"The average silhouette_score is :\", sil_score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea2a9ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "range_n_clusters = [2,3,4,5,6,7,8,9]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(std_df) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    cluster_labels = clusterer.fit_predict(std_df)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = sil(std_df, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(std_df, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(std_df[:,0], std_df[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:,0], centers[:,1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data after Standard scaler.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98181c5",
   "metadata": {},
   "source": [
    "####  Conclusion:\n",
    "According the the silhouette score of:\n",
    "\n",
    "The standardized data, although the ideal number of clusters is 2, with a score higher than other options, of 0.40. we will go for 3 number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bebdfe0",
   "metadata": {},
   "source": [
    "## 6.3 Build KMeans Cluster algorithm using K=3 and Standard Scaler Applied Dataset <a class=\"anchor\" id=\"6.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8e3ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have found good number of cluster = 3\n",
    "# model building using cluster numbers = 3\n",
    "\n",
    "model_kmeans = KMeans(n_clusters=3, random_state=0, init='k-means++')\n",
    "y_predict_kmeans = model_kmeans.fit_predict(std_df)\n",
    "y_predict_kmeans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c16f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are nothing but cluster labels...\n",
    "\n",
    "y_predict_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53930f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ee7438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster centres associated with each lables\n",
    "\n",
    "model_kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e8c058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# within-cluster sum of squared\n",
    "\n",
    "# The lower values of inertia are better and zero is optimal.\n",
    "# Inertia is the sum of squared error for each cluster. \n",
    "# Therefore the smaller the inertia the denser the cluster(closer together all the points are)\n",
    "\n",
    "model_kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a356c6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign clusters to the data set\n",
    "df['Kmeans_label'] = model_kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab3d8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by Clusters (K=6)\n",
    "df.groupby('Kmeans_label').agg(['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843fb712",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Kmeans_label']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20dccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Kmeans_label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d450eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Kmeans_label']==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590243a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(1,2,sharey=False)\n",
    "fig.set_size_inches(15,6)\n",
    "\n",
    "\n",
    "\n",
    "sil_visualizer1 = SilhouetteVisualizer(model_kmeans,ax= ax1, colors=['#922B21','#5B2C6F','#1B4F72','#32a84a'])\n",
    "sil_visualizer1.fit(std_df)\n",
    "\n",
    "\n",
    "# 2nd Plot showing the actual clusters formed\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "colors1 = cm.nipy_spectral(model_kmeans.labels_.astype(float) / 3) # 6 is number of clusters\n",
    "ax2.scatter(std_df[:, 0], std_df[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors1, edgecolor='k')\n",
    "\n",
    "# Labeling the clusters\n",
    "centers1 = model_kmeans.cluster_centers_\n",
    "# Draw white circles at cluster centers\n",
    "ax2.scatter(centers1[:, 0], centers1[:, 1], marker='o',c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "for i, c in enumerate(centers1):\n",
    "    ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,s=50, edgecolor='k')\n",
    "\n",
    "\n",
    "ax2.set_title(label =\"The visualization of the clustered data.\")\n",
    "ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % 3),fontsize=14, fontweight='bold')\n",
    "\n",
    "sil_visualizer1.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad4ca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the data we reduced to 2 sets.\n",
    "\n",
    "plt.scatter(std_df[:,0], std_df[:,1], c = model_kmeans.labels_, s = 50, cmap = \"viridis\")\n",
    "\n",
    "centers = model_kmeans.cluster_centers_                                 \n",
    "# We want to create 2 centers and show them on the visual.\n",
    "\n",
    "plt.scatter(centers[:,0], centers[:,1], c = \"black\", s = 200, alpha = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465647f5",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "+ #### The Cities in the Cluster-0 seems to be Safe-Zone where there are relativley less Murders,Assaults and Rapes.\n",
    "+ #### The Cities in Cluster-1 seems to have higher crime rates and can be regarded as Danger-Zone.\n",
    "+ #### The Cities in Cluster-2 seems to have Moderate crime rates and can be regarded as Moderate-Zone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5083b89",
   "metadata": {},
   "source": [
    "### 6.4 Elbow Method and Silhouette Score on MinMaxScaler Applied Data<a class=\"anchor\" id=\"6.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2d5cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_range = range(1,15)\n",
    "cluster_errors = []\n",
    "for num_clusters in cluster_range:\n",
    "    clusters = KMeans(num_clusters,n_init=10)\n",
    "    clusters.fit(minmax_df)\n",
    "    labels = clusters.labels_\n",
    "    centroids = clusters.cluster_centers_\n",
    "    cluster_errors.append(clusters.inertia_)\n",
    "clusters_df = pd.DataFrame({\"num_clusters\":cluster_range,\"cluster_errors\":cluster_errors})\n",
    "clusters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b75ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss=[]\n",
    "for i in range (1,9):\n",
    "    kmeans=KMeans(n_clusters=i,random_state=2)\n",
    "    kmeans.fit(minmax_df)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "# Plot K values range vs WCSS to get Elbow graph for choosing K (no. of clusters)\n",
    "plt.plot(range(1,9),wcss,color = 'black')\n",
    "plt.scatter(range(1,9),wcss,color='red')\n",
    "plt.title('Elbow Graph for MinMaxScaler')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9202d5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a scikit-learn K-Means model. we will check for two diff hyperparameters value effect.\n",
    "model = KMeans(random_state=10, max_iter=500, init='k-means++')\n",
    "\n",
    "# Instantiate the KElbowVisualizer with the number of clusters and the metric\n",
    "visualizer = KElbowVisualizer(model, k=(2,20), metric='silhouette', timings=False)\n",
    "# Fit the data and visualize\n",
    "print('Elbow Plot for MinMaxScaler data')\n",
    "visualizer.fit(minmax_df)    \n",
    "visualizer.poof()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991ddd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the elbow method, the ideal number of clusters to use was 6.\n",
    "# We will also use the Silhouette score to determine an optimal number.\n",
    "\n",
    "clust_list = [2,3,4,5,6,7,8,9]\n",
    "\n",
    "#  Silhouette score for MinMaxScaler Applied on data .\n",
    "\n",
    "for n_clusters in clust_list:\n",
    "    clusterer1 = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    cluster_labels1 = clusterer1.fit_predict(minmax_df)\n",
    "    sil_score1= sil(minmax_df, cluster_labels1)\n",
    "    print(\"For n_clusters =\", n_clusters,\"The average silhouette_score is :\", sil_score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb55b36",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "range_n_clusters = [2,3,4,5,6,7,8,9]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(minmax_df) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(minmax_df)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = sil(minmax_df, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(minmax_df, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(minmax_df[:,0], minmax_df[:,1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:,0], centers[:,1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data after Standard scaler.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca52e366",
   "metadata": {},
   "source": [
    "####  Conclusion:\n",
    "According the the silhouette score of:\n",
    "\n",
    "+ The MinMax Scaler Applied data, the ideal number of clusters is 2, with a score higher than other options, of 0.42, but if you notice the number of cluster 4 has also the best variation within cluster and between cluster with zero to no values going towards -1 and no mismatch in cluster segregation than the consecutive rest\n",
    "\n",
    "+ If we check silhouette score with Normalize data kmeans model 0.42>0.40 .Best score always close to +1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b00dcd",
   "metadata": {},
   "source": [
    "## 6.5 Build KMeans Cluster algorithm using K=2 and MinMaxScaler Applied Dataset<a class=\"anchor\" id=\"6.5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066290f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have found good number of cluster = 2\n",
    "# model building using cluster numbers = 2\n",
    "\n",
    "model_kmeans = KMeans(n_clusters=2, random_state=0, init='k-means++')\n",
    "y_predict_kmeans = model_kmeans.fit_predict(minmax_df)\n",
    "y_predict_kmeans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52819a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are nothing but cluster labels...\n",
    "\n",
    "y_predict_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b72dcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82e5d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster centres associated with each lables\n",
    "\n",
    "model_kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f639f152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# within-cluster sum of squared\n",
    "\n",
    "# The lower values of inertia are better and zero is optimal.\n",
    "# Inertia is the sum of squared error for each cluster. \n",
    "# Therefore the smaller the inertia the denser the cluster(closer together all the points are)\n",
    "\n",
    "model_kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bed20d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign clusters to the data set\n",
    "df['Kmeans_label'] = model_kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8635321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(1,2,sharey=False)\n",
    "fig.set_size_inches(15,6)\n",
    "\n",
    "\n",
    "\n",
    "sil_visualizer1 = SilhouetteVisualizer(model_kmeans,ax= ax1, colors=['#922B21','#5B2C6F','#1B4F72','#32a84a'])\n",
    "sil_visualizer1.fit(minmax_df)\n",
    "\n",
    "\n",
    "# 2nd Plot showing the actual clusters formed\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "colors1 = cm.nipy_spectral(model_kmeans.labels_.astype(float) / 4) # 6 is number of clusters\n",
    "ax2.scatter(minmax_df[:, 0], minmax_df[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors1, edgecolor='k')\n",
    "\n",
    "# Labeling the clusters\n",
    "centers1 = model_kmeans.cluster_centers_\n",
    "# Draw white circles at cluster centers\n",
    "ax2.scatter(centers1[:, 0], centers1[:, 1], marker='o',c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "for i, c in enumerate(centers1):\n",
    "    ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,s=50, edgecolor='k')\n",
    "\n",
    "\n",
    "ax2.set_title(label =\"The visualization of the clustered data.\")\n",
    "ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % 2),fontsize=14, fontweight='bold')\n",
    "\n",
    "sil_visualizer1.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81254e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the data we reduced to 2 sets.\n",
    "\n",
    "plt.scatter(minmax_df[:,0], minmax_df[:,1], c = model_kmeans.labels_, s = 50, cmap = \"viridis\")\n",
    "\n",
    "centers = model_kmeans.cluster_centers_                                 \n",
    "# We want to create 2 centers and show them on the visual.\n",
    "\n",
    "plt.scatter(centers[:,0], centers[:,1], c = \"black\", s = 200, alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc875070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by Clusters (K=6)\n",
    "df.groupby('Kmeans_label').agg(['mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7643809",
   "metadata": {},
   "source": [
    "#### We can now clearly see the 2 number of clusters formed, which can be described as under\n",
    "\n",
    "+ Cluster0, Safe Zone Defines the group of Urban States with Lowest of Crimes\n",
    "+ Cluster1, Danger Zone Defines the group of Urban States with Highest Rate of Crimes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c420272",
   "metadata": {},
   "source": [
    "## 7 Hierarchical Clustering Algorithm<a class=\"anchor\" id=\"7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0824c9",
   "metadata": {},
   "source": [
    "#### 7.1 Dendogram on MinMaxScaler Applied on Dataset<a class=\"anchor\" id=\"7.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9512704",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Applying Dendrogram on data. Or you may apply it on Standardized/normalized indepedent variable data.\n",
    "# Here diffrent linkage method from hyperparameter is used to see diff between methods for understanding. \n",
    "# Ward method is commanly used since it is simpler to visualize understanding.\n",
    "# Find number of cluster's using color coding of dendrogram. Each color indicates one cluster.\n",
    "\n",
    "for methods in ['single','complete','average','weighted','centroid','median','ward']: \n",
    "    plt.figure(figsize =(20, 6)) \n",
    "    \n",
    "    dict = {'fontsize':24,'fontweight' :16, 'color' : 'blue'}\n",
    "    \n",
    "    plt.title('Visualising the data, Method- {}'.format(methods),fontdict = dict) \n",
    "    Dendrogram1 = sch.dendrogram(sch.linkage(minmax_df, method = methods,optimal_ordering=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7f9103",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = [2,3,4,5,6,7,8]  # always start number from 2.\n",
    "\n",
    "for n_clusters in n_clusters:\n",
    "    for linkages in [\"ward\", \"complete\", \"average\", \"single\"]:\n",
    "        hie_cluster1 = AgglomerativeClustering(n_clusters=n_clusters,linkage=linkages) # bydefault it takes linkage 'ward'\n",
    "        hie_labels1 = hie_cluster1.fit_predict(minmax_df)\n",
    "        silhouette_score1 = sil(minmax_df, hie_labels1)\n",
    "        print(\"For n_clusters =\", n_clusters,\"The average silhouette_score with linkage-\",linkages, ':',silhouette_score1)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b5758b",
   "metadata": {},
   "source": [
    "#### The seperation in Each Method indicate that 2 as the optimal number for clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfb5c59",
   "metadata": {},
   "source": [
    "### 7.2 Dendrogram on Standard Scaler Applied on Data<a class=\"anchor\" id=\"7.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d4e9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = [2,3,4,5,6,7,8]  # always start number from 2.\n",
    "\n",
    "for n_clusters in n_clusters:\n",
    "    for linkages in [\"ward\", \"complete\", \"average\", \"single\"]:\n",
    "        hie_cluster1 = AgglomerativeClustering(n_clusters=n_clusters,linkage=linkages) # bydefault it takes linkage 'ward'\n",
    "        hie_labels1 = hie_cluster1.fit_predict(std_df)\n",
    "        silhouette_score1 = sil(std_df, hie_labels1)\n",
    "        print(\"For n_clusters =\", n_clusters,\"The average silhouette_score with linkage-\",linkages, ':',silhouette_score1)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4626071d",
   "metadata": {},
   "source": [
    "<b>Conclusion</b>: Max score is with cluster numbers 2 in both standard scaler transformation and Min Max scaler transformation. \n",
    "\n",
    "+ Heirarchical clustering means creating a tree of clusters by iteratively grouping or separating data points. There are two types of hierarchical clustering: Agglomerative clustering Divisive clustering We now apply the Agglomerative clustering technique:Agglomerative clustering is kind of a bottom-up approach. Each data point is assumed to be a separate cluster at first. Then the similar clusters are iteratively combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27756346",
   "metadata": {},
   "source": [
    "### 7.3 Run Hierarchical Clustering.(Agglomerative Clustering) <a class=\"anchor\" id=\"7.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733c2d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_clustering = AgglomerativeClustering(n_clusters=2, linkage='ward')\n",
    "y_pred_hie = agg_clustering.fit_predict(minmax_df)\n",
    "print(y_pred_hie.shape)\n",
    "y_pred_hie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188a8d08",
   "metadata": {},
   "source": [
    "<b>Ward</b> method is actually a method that tries to minimize the variance within each cluster. In K-means when we were trying to minimize the wcss to plot our elbow method chart, here its almost the same the only difference is that instead of minimizing wcss we are minimizing the within-cluster variants. That is the variance within each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99e878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster numbers\n",
    "\n",
    "agg_clustering.n_clusters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df8444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering Score\n",
    "\n",
    "(sil(minmax_df, agg_clustering.labels_)*100).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f540ff9a",
   "metadata": {},
   "source": [
    "### Putting Cluster lables into original dataset And analysis of the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36934c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concating Labels with main dataset copy\n",
    "\n",
    "df['Hierarchical_labels'] = agg_clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06b4a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Hierarchical_labels').agg(['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ebe5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.boxplot(x='Hierarchical_labels', y='Murder', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929135cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.boxplot(x='Hierarchical_labels', y='Assault', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab04d0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.boxplot(x='Hierarchical_labels', y='Rape', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008edf70",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "+ #### The Cities in the Cluster-0 seems to be Safe-Zone where there are relativley less Murders,Assaults and Rapes.\n",
    "+ #### The Cities in Cluster-1 seems to have higher crime rates and can be regarded as Danger-Zone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b3d4f9",
   "metadata": {},
   "source": [
    "## 8. DBSCAN - (Density Based Spatial Clustering of Applications with Noise)<a class=\"anchor\" id=\"8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ba3e8a",
   "metadata": {},
   "source": [
    "##### Core  This is a point that has at least m points within distance n from itself.\n",
    "##### Border  This is a point that has at least one Core point at a distance n.\n",
    "##### Noise  This is a point that is neither a Core nor a Border. And it has less than m points within distance n from itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b96dac8",
   "metadata": {},
   "source": [
    "### 8.1 DBSCAN of Standard Scaled Data<a class=\"anchor\" id=\"8.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b05d029",
   "metadata": {},
   "source": [
    "##### To choose the best combination of the algorithm parameters I will first create a matrix of investigated combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee817b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "eps_values = np.arange(0.25,2,0.25) # eps values to be investigated\n",
    "min_samples = np.arange(1,3)# min_samples values to be investigated\n",
    "DBSCAN_params = list(product(eps_values, min_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc666d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_clusters = []\n",
    "sil_score = []\n",
    "\n",
    "for p in DBSCAN_params:\n",
    "    DBS_clustering = DBSCAN(eps=p[0], min_samples=p[1]).fit(std_df)\n",
    "    no_of_clusters.append(len(np.unique(DBS_clustering.labels_)))\n",
    "    sil_score.append(sil(std_df, DBS_clustering.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5784cf57",
   "metadata": {},
   "source": [
    "##### Collecting number of generated clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e869fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame.from_records(DBSCAN_params, columns =['Eps', 'Min_samples'])   \n",
    "tmp['No_of_clusters'] = no_of_clusters\n",
    "\n",
    "pivot_1 = pd.pivot_table(tmp, values='No_of_clusters', index='Min_samples', columns='Eps')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "sns.heatmap(pivot_1, annot=True,annot_kws={\"size\": 16}, cmap=\"YlGnBu\", ax=ax)\n",
    "ax.set_title('Number of clusters')\n",
    "print('A heatplot below shows how many clusters were genreated by the algorithm for the respective parameters combinations.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab68844",
   "metadata": {},
   "source": [
    "##### As the heatplot above shows, the number of clusters vary from 49 to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98715e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame.from_records(DBSCAN_params, columns =['Eps', 'Min_samples'])   \n",
    "tmp['Sil_score'] = sil_score\n",
    "\n",
    "pivot_1 = pd.pivot_table(tmp, values='Sil_score', index='Min_samples', columns='Eps')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18,6))\n",
    "sns.heatmap(pivot_1, annot=True, annot_kws={\"size\": 10}, cmap=\"YlGnBu\", ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17f92b7",
   "metadata": {},
   "source": [
    "##### Global maximum is 0.23 for eps=1.25 and min_samples=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e82ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = [0.25,0.50,0.75,1,1.25,1.50,1.75,2]\n",
    "min_samples = [1,2]\n",
    "\n",
    "\n",
    "sil_avg = []\n",
    "max_value = [0,0,0,0]\n",
    "\n",
    "for i in range(len(epsilon)):\n",
    "    for j in range(len(min_samples)):\n",
    "\n",
    "        db = DBSCAN(min_samples = min_samples[j], eps =epsilon[i]).fit(std_df)\n",
    "        #cluster_labels=dbscan.fit_predict(data) \n",
    "        core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "        core_samples_mask[db.core_sample_indices_] = True\n",
    "        labels = db.labels_\n",
    "\n",
    "        # Number of clusters in labels, ignoring noise if present.\n",
    "        n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise_ = list(labels).count(-1)\n",
    "\n",
    "\n",
    "        silhouette_avg = sil(std_df, labels)\n",
    "        if silhouette_avg > max_value[3]:\n",
    "            max_value=(epsilon[i], min_samples[j], n_clusters_, silhouette_avg)\n",
    "        sil_avg.append(silhouette_avg)\n",
    "\n",
    "print(\"epsilon=\", max_value[0], \n",
    "      \"\\nmin_sample=\", max_value[1],\n",
    "      \"\\nnumber of clusters=\", max_value[2],\n",
    "      \"\\naverage silhouette score= %.4f\" % max_value[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f685f700",
   "metadata": {},
   "source": [
    "##### Run DBSCAN on Standard Scaled data with optimal min_sample and epsilon values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492715cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=1.5, min_samples=1) # min_samples = number of columns  or len(dataset.columns)* 3\n",
    "dbscan.fit(std_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed80515",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63077932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -1 are the noise points in our dataset and the rest are the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff4fd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concating Labels with main dataset copy\n",
    "\n",
    "df['DBSCAN_labels'] = dbscan.labels_\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1aed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('DBSCAN_labels').agg(['mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17950eb4",
   "metadata": {},
   "source": [
    "### Silhouette Score for DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07698cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Silhouette Coefficient: %0.3f\" % sil(std_df, dbscan.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51bbdd1",
   "metadata": {},
   "source": [
    "### 8.2 DBSCAN on MinMax Scaled Data<a class=\"anchor\" id=\"8.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a846cc8",
   "metadata": {},
   "source": [
    "##### To choose the best combination of the algorithm parameters I will first create a matrix of investigated combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4526e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "eps_values = np.arange(0.25,0.75,0.25) # eps values to be investigated\n",
    "min_samples = np.arange(1,3)# min_samples values to be investigated\n",
    "DBSCAN_params = list(product(eps_values, min_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caf345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_clusters = []\n",
    "sil_score = []\n",
    "\n",
    "for p in DBSCAN_params:\n",
    "    DBS_clustering = DBSCAN(eps=p[0], min_samples=p[1]).fit(minmax_df)\n",
    "    no_of_clusters.append(len(np.unique(DBS_clustering.labels_)))\n",
    "    sil_score.append(sil(minmax_df, DBS_clustering.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860cb3f3",
   "metadata": {},
   "source": [
    "##### Collecting number of generated clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db99bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame.from_records(DBSCAN_params, columns =['Eps', 'Min_samples'])   \n",
    "tmp['No_of_clusters'] = no_of_clusters\n",
    "\n",
    "pivot_1 = pd.pivot_table(tmp, values='No_of_clusters', index='Min_samples', columns='Eps')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "sns.heatmap(pivot_1, annot=True,annot_kws={\"size\": 16}, cmap=\"YlGnBu\", ax=ax)\n",
    "ax.set_title('Number of clusters')\n",
    "print('A heatplot below shows how many clusters were genreated by the algorithm for the respective parameters combinations.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78502097",
   "metadata": {},
   "source": [
    "##### As the heatplot above shows, the number of clusters vary from 14 to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06013f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame.from_records(DBSCAN_params, columns =['Eps', 'Min_samples'])   \n",
    "tmp['Sil_score'] = sil_score\n",
    "\n",
    "pivot_1 = pd.pivot_table(tmp, values='Sil_score', index='Min_samples', columns='Eps')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18,6))\n",
    "sns.heatmap(pivot_1, annot=True, annot_kws={\"size\": 10}, cmap=\"YlGnBu\", ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7729dd",
   "metadata": {},
   "source": [
    "##### Global maximum is 0.21 for eps=0.5 and min_samples=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb914094",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = [0.25,0.5]\n",
    "min_samples = [1,2]\n",
    "\n",
    "\n",
    "sil_avg = []\n",
    "max_value = [0,0,0,0]\n",
    "\n",
    "for i in range(len(epsilon)):\n",
    "    for j in range(len(min_samples)):\n",
    "\n",
    "        db = DBSCAN(min_samples = min_samples[j], eps =epsilon[i]).fit(minmax_df)\n",
    "        #cluster_labels=dbscan.fit_predict(data) \n",
    "        core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "        core_samples_mask[db.core_sample_indices_] = True\n",
    "        labels = db.labels_\n",
    "\n",
    "        # Number of clusters in labels, ignoring noise if present.\n",
    "        n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise_ = list(labels).count(-1)\n",
    "\n",
    "\n",
    "        silhouette_avg = sil(minmax_df, labels)\n",
    "        if silhouette_avg > max_value[3]:\n",
    "            max_value=(epsilon[i], min_samples[j], n_clusters_, silhouette_avg)\n",
    "        sil_avg.append(silhouette_avg)\n",
    "\n",
    "print(\"epsilon=\", max_value[0], \n",
    "      \"\\nmin_sample=\", max_value[1],\n",
    "      \"\\nnumber of clusters=\", max_value[2],\n",
    "      \"\\naverage silhouette score= %.4f\" % max_value[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade9e628",
   "metadata": {},
   "source": [
    "##### Run DBSCAN on Standard Scaled data with optimal min_sample and epsilon values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267f7e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.5, min_samples=1) # min_samples = number of clumns * 3\n",
    "dbscan.fit(minmax_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282cbd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee7ac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -1 are the noise points in our dataset and the rest are the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42f8a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concating Labels with main dataset copy\n",
    "\n",
    "df['DBSCAN_labels'] = dbscan.labels_\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3206dc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('DBSCAN_labels').agg(['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d44db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting elements based on cluster label assigned and taking average for insights.\n",
    "\n",
    "cluster1 = pd.DataFrame(df.loc[df.DBSCAN_labels==0].mean(),columns= ['Cluster1_avg'])\n",
    "cluster2 = pd.DataFrame(df.loc[df.DBSCAN_labels==1].mean(),columns= ['Cluster2_avg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8aa694",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_df = pd.concat([cluster1,cluster2],axis=1)\n",
    "avg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1006ea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and plot one Column data .xs method\n",
    "for i , row in avg_df.iterrows():\n",
    "    fig = plt.subplots(figsize=(8,6))\n",
    "    j = avg_df.xs(i ,axis = 0)\n",
    "    plt.title(i, fontsize=16, fontweight=20)\n",
    "    j.plot(kind='bar',fontsize=14)\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da699d37",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "+ #### The Cities in the Cluster-0 seems to be Safe-Zone where there are relativley less Murders,Assaults and Rapes.\n",
    "+ #### The Cities in Cluster-1 seems to have higher crime rates and can be regarded as Danger-Zone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c8e595",
   "metadata": {},
   "source": [
    "# 9. Conclusion:<a class=\"anchor\" id=\"9\"></a>\n",
    "\n",
    "##### I have applied EDA to analyze dataset.Discovered correlation between diff variables and found colinearity.\n",
    "##### Applied Standardazation & MinMaxScalar transformation on the data t\n",
    "##### I have used & analyzed two clustering techniques here..i) KMeans, ii) Hierarchical Clusterig & iii) DBSCAN.\n",
    "\n",
    "##### By applying clustering on diff. PCA obtained with diff transformation data shows fluctuation in model score. So finally the Standard Scaler found less score so not used for further model building.\n",
    "\n",
    "##### KMeans clustering is sensitive to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52090650",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
